{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "282157ae-2d20-4fa9-8ac9-8bf92ab6d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from torch import distributed, nn\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, HTTPException, Request, Response, Form\n",
    "from contextlib import asynccontextmanager\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List, Literal, Optional, Union\n",
    "import time \n",
    "import numpy as np \n",
    "import logging\n",
    "import uuid\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e69ab6-d68e-4b6a-881e-c96f216122b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.15it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "\n",
    "MODEL_PATH = \"/root/test/model_zoo/Qwen1.5-7B-Chat\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "# async def translate_prompt(prompt):\n",
    "#     prefix = '请把下面一段话翻译成英文；如果原本就是英文那么请直接输出原文：\\n'\n",
    "#     full_prompt = prefix + prompt + \"\\n翻译成英文结果：\"\n",
    "#     print(full_prompt)\n",
    "#     inputs = tokenizer(full_prompt,return_tensors='pt').to('cuda')\n",
    "#     generate_ids = model.generate(inputs.input_ids, generation_config=generation_config)\n",
    "#     pred = tokenizer.batch_decode(generate_ids,skip_special_tokens=True)\n",
    "#     print(\"\\npred:\\n\")\n",
    "#     print(pred)\n",
    "#     return pred[0].split(\"翻译成英文结果：\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf98fab3-f26f-4d50-a8fd-6c73eac57325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_prompt(prompt):\n",
    "    prefix = '请把下面一段话翻译成英文；如果原本就是英文那么请直接输出原文：\\n'\n",
    "    full_prompt = prefix + prompt + \"\\n翻译成英文结果：\"\n",
    "    print(full_prompt)\n",
    "    inputs = tokenizer(full_prompt,return_tensors='pt').to('cuda')\n",
    "    generate_ids = model.generate(inputs.input_ids, generation_config=generation_config)\n",
    "    pred = tokenizer.batch_decode(generate_ids,skip_special_tokens=True)\n",
    "    print(\"\\npred:\\n\")\n",
    "    print(pred)\n",
    "    return pred[0].split(\"翻译成英文结果：\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ee445ff-e408-436e-b248-8b6d6d52b9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请把下面一段话翻译成英文；如果原本就是英文那么请直接输出原文：\n",
      "кот играет в снегу\n",
      "翻译成英文结果：\n",
      "\n",
      "pred:\n",
      "\n",
      "['请把下面一段话翻译成英文；如果原本就是英文那么请直接输出原文：\\nкот играет в снегу\\n翻译成英文结果：']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_prompt(\"кот играет в снегу\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf495c0-bf01-4245-95c7-331b1cb2035e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dcdd151-e30c-4ff3-9a74-c6e3291c5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "class Translator:\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = MarianMTModel.from_pretrained(model_path)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        # self.pipeline = pipeline(\"translation\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def translate(self, text):\n",
    "        model_inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        translated_outputs = self.model.generate(**model_inputs, num_beams=10)\n",
    "        return [self.tokenizer.decode(_, skip_special_tokens=True) for _ in translated_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f772bf08-e17b-4f4b-bcb6-9379d92e858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Translator(\"/root/test/nlp/mt_zh2en/mt_zh2en/multi_en_models/opus-mt-ja-en\")\n",
    "# res = model.translate([\"有时候，有时候，我会\", \"你是一个好人\"])\n",
    "# res = model.translate([\"sometimes, sometimes, I would\", \"you are a good guy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c74d4a3a-6700-4553-a00a-1077524081ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a086571-863f-43b0-ad7a-cb1591f450e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.translate([\"こんにちはようこそ\", \"雪の中で遊ぶ猫\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29c900e1-e30d-49b7-9997-98ed1dc31426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello. Welcome.', \"It's a cat playing in the snow.\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e2e17-0b38-4f89-b231-8db2e34b8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "python main2.py --trmodelpath /root/test/nlp/mt_zh2en/mt_zh2en/multi_en_models/opus-mt-fr-en"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
